{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. S. Harmeling, SS 2024\n",
    "\n",
    "### Exercise sheet #4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implement Monte Carlo prediction.\n",
    "\n",
    "The idea of Monte Carlo prediction is very simple: Estimate the value (or the action value) by averating the observed returns from collected episodes. In this notebook we apply Monte Carlo prediction to the game of tic-tac-toe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Make sure that the file `rl_env.py` is in the same folder as the notebook (the same file as in exercise sheet #2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import rl_env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already implemented tic-tac-toe in `TicTacToeEnv`:\n",
    "- The environment has $3^9 = 19683$ states (9 fields with 3 values: empty, player 1, player 2).\n",
    "- There are $9$ possible actions, which determine the next move of the current player (i.e. the actions control both players).\n",
    "- The final reward is $1$ if player 1 wins, and $0$ if player 2 wins or when there is a draw. The reward is $0$ in all other time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the tic-tac-toe environment\n",
    "env = rl_env.TicTacToeEnv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already implemented the random policy for the tic-tac-toe environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    # Obtain the list of empty fields\n",
    "    valid_actions = rl_env.TicTacToeEnv.get_valid_actions(state)\n",
    "    # Select one of the empty fields randomly\n",
    "    # For non-empty fields, the action does not have an effect\n",
    "    action = np.random.choice(valid_actions)\n",
    "    return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement Monte Carlo prediction of the action value for the **initial state**, i.e. you don't need to compute the action values for all states, but only the $9$ action values for the initial state.  \n",
    "We don't need a discount factor, so the initial return is equal to the final reward.\n",
    "\n",
    "You don't need an `Agent` object for this implementation, just generate episodes and estimate the action values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# TODO: Implement Monte Carlo prediction of the action value function #\n",
    "# for the initial state as described above. Generate at least 10000   #\n",
    "# episodes to estimate the action values.                             #\n",
    "#######################################################################\n",
    "# Create arrays for total returns and counts for the initial state\n",
    "G = np.zeros(9, dtype=float)\n",
    "N = np.zeros(9, dtype=int)\n",
    "\n",
    "# Generate 10000 episodes\n",
    "for i in range(10000):\n",
    "    # Reset the environment\n",
    "    state, _ = env.reset()\n",
    "    # Remember the start action to update the action value\n",
    "    start_action = random_policy(state)\n",
    "\n",
    "    # Rollout the episode\n",
    "    action = start_action\n",
    "    while True:\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        action = random_policy(state)\n",
    "\n",
    "    if not truncated:\n",
    "        # We cannot use the episode if it was truncated, since we cannot\n",
    "        # compute the true returns (not important for this environment)\n",
    "\n",
    "        # No rewards until the end and no discounting,\n",
    "        # so the initial return is equal to final reward\n",
    "        g = reward\n",
    "        # Update the total return and count for the initial state\n",
    "        G[start_action] += g\n",
    "        N[start_action] += 1\n",
    "\n",
    "# Compute the action values\n",
    "q = G / N\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# End of your code.                                                   #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61717352, 0.5328084 , 0.60154242, 0.55996223, 0.68726937,\n",
       "       0.53946147, 0.62697674, 0.53953084, 0.60834813])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the reward is only $1$ if player 1 wins, the value of the initial state is equal to the winning probability of player 1.  \n",
    "Use this to answer the following questions:\n",
    "- What is the probability that the first player wins?\n",
    "- Which initial action has the highest chance of winning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total returns: 690.0 609.0 702.0 593.0 745.0 581.0 674.0 621.0 685.0\n",
      "counts: 1118 1143 1167 1059 1084 1077 1075 1151 1126\n",
      "q: 0.617 0.533 0.602 0.560 0.687 0.539 0.627 0.540 0.608\n",
      "v: 0.590\n",
      "average win chance: 59.03%\n",
      "best start action: 4\n",
      "win chance of best start action: 68.73%\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# TODO: Answer the questions by using the computed action values.     #\n",
    "#######################################################################\n",
    "# Convert the action values to state values\n",
    "# Since all actions in the initial state have uniform probability (1/9),\n",
    "# we can just calculate the mean\n",
    "# v(s) = \\sum_a \\pi(a|s) q(s,a) = 1/9 * \\sum_a q(s,a)\n",
    "v = q.mean()\n",
    "\n",
    "print('total returns:', *[f'{x:.1f}' for x in G])\n",
    "print('counts:', *N)\n",
    "print('q:', *[f'{x:.3f}' for x in q])\n",
    "print('v:', f'{v:.3f}')\n",
    "print('average win chance:', f'{v * 100:.2f}%')\n",
    "print('best start action:', np.argmax(q))\n",
    "print('win chance of best start action:', f'{np.max(q) * 100:.2f}%')\n",
    "\n",
    "# Action 4 is the field in the middle of the board\n",
    "#######################################################################\n",
    "# End of your code.                                                   #\n",
    "#######################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
